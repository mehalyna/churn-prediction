{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Machine Learning with Utility Functions**\n",
    "\n",
    "### **Approach A: Cost-sensitive Classification using Decision Trees**\n",
    "\n",
    "### **Objective:**\n",
    "Implement a cost-sensitive decision tree classifier that explicitly integrates **customer lifetime value (CLV)** and intervention costs into the model's predictions, optimizing decision-making to maximize retention and minimize unnecessary promotional spending.\n",
    "\n",
    "### **How the approach works:**\n",
    "\n",
    "- **Cost-Sensitive Classification** involves assigning different \"costs\" (or utilities) to different prediction outcomes:\n",
    "  \n",
    "| Prediction / Reality | Churn (True)               | Stay (False)                 |\n",
    "|----------------------|----------------------------|------------------------------|\n",
    "| **Predict Churn**    | **High Utility (CLV - Cost)** | **Negative Cost (wasted marketing spend)** |\n",
    "| **Predict Stay**     | **Negative (lost CLV)** | **Neutral (No intervention needed)**   |\n",
    "\n",
    "- The model learns to prioritize decisions where potential losses (losing high-value customers) are minimized and utility (profitability from retention) is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation of the provided implementation:**\n",
    "\n",
    "- **Step-by-step Data Preparation**:\n",
    "  - Clearly encodes categorical variables and creates essential engineered features.\n",
    "  - Calculates explicit utility values based on customer lifetime value (CLV).\n",
    "\n",
    "- **Cost-sensitive Decision Tree Classifier**:\n",
    "  - Trains the model using `sample_weight`, focusing the modelâ€™s learning more heavily on customers who represent higher potential gains or losses (utility-driven).\n",
    "\n",
    "- **Model Evaluation**:\n",
    "  - Clearly interprets performance using classification report and confusion matrix.\n",
    "  - Calculates explicit **total utility** to determine the financial effectiveness of predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Expected Practical Outcome**:\n",
    "\n",
    "- Immediately identifies high-utility customers for targeted intervention.\n",
    "- Reduces unnecessary promotional spending.\n",
    "- Maximizes retained customer lifetime value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Complete Practical Implementation in Python:**\n",
    "\n",
    "Below is the full, clearly explained Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for modeling: ['Return_Ratio', 'Purchase_Frequency', 'CLV', 'Gender_Male', 'Gender_Other', 'Promotion_Response_Responded', 'Promotion_Response_Unsubscribed', 'Email_Opt_In_True']\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49       135\n",
      "           1       0.57      0.55      0.56       165\n",
      "\n",
      "    accuracy                           0.53       300\n",
      "   macro avg       0.52      0.52      0.52       300\n",
      "weighted avg       0.53      0.53      0.53       300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[67 68]\n",
      " [74 91]]\n",
      "\n",
      "Total Utility (financial impact) of model decisions: $49879.37\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "data = pd.read_csv('online_retail_customer_churn.csv')\n",
    "\n",
    "# Step 2: Encode categorical features safely\n",
    "data_encoded = pd.get_dummies(\n",
    "    data, \n",
    "    columns=['Gender', 'Promotion_Response', 'Email_Opt_In'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "data_encoded['Return_Ratio'] = data_encoded['Num_of_Returns'] / data_encoded['Num_of_Purchases'].replace(0, 1)\n",
    "data_encoded['Purchase_Frequency'] = data_encoded['Num_of_Purchases'] / data_encoded['Last_Purchase_Days_Ago'].replace(0, 1)\n",
    "data_encoded['CLV'] = (data_encoded['Average_Transaction_Amount'] *\n",
    "                       data_encoded['Purchase_Frequency'] *\n",
    "                       data_encoded['Years_as_Customer']).fillna(0)\n",
    "\n",
    "# Estimated intervention cost (e.g., marketing cost per offer)\n",
    "data_encoded['Intervention_Cost'] = 10  # Assumed fixed cost per intervention\n",
    "\n",
    "# Step 4: Define utility values explicitly\n",
    "data_encoded['Utility_True_Positive'] = data_encoded['CLV'] - data_encoded['Intervention_Cost']\n",
    "data_encoded['Utility_False_Positive'] = - data_encoded['Intervention_Cost']\n",
    "data_encoded['Utility_False_Negative'] = - data_encoded['CLV']\n",
    "data_encoded['Utility_True_Negative'] = 0\n",
    "\n",
    "# Step 5: Dynamically identify features for modeling\n",
    "encoded_columns = [col for col in data_encoded.columns if \n",
    "                   col.startswith('Gender_') or\n",
    "                   col.startswith('Promotion_Response_') or\n",
    "                   col.startswith('Email_Opt_In_')]\n",
    "\n",
    "features = ['Return_Ratio', 'Purchase_Frequency', 'CLV'] + encoded_columns\n",
    "\n",
    "# Display the selected features clearly\n",
    "print(\"Selected features for modeling:\", features)\n",
    "\n",
    "# Step 6: Split dataset into training and testing sets\n",
    "X = data_encoded[features]\n",
    "y = data_encoded['Target_Churn'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test, util_train, util_test = train_test_split(\n",
    "    X, y,\n",
    "    data_encoded[['Utility_True_Positive', 'Utility_False_Positive', \n",
    "                  'Utility_False_Negative', 'Utility_True_Negative']],\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 7: Train cost-sensitive Decision Tree model\n",
    "# Weights are proportional to the absolute utility to emphasize high-impact decisions\n",
    "sample_weights = np.where(\n",
    "    y_train == 1,\n",
    "    util_train['Utility_True_Positive'].abs(),\n",
    "    util_train['Utility_False_Negative'].abs()\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Step 8: Model prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate model performance clearly\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Step 10: Calculate total realized utility of the model predictions\n",
    "TP = (y_pred == 1) & (y_test == 1)\n",
    "FP = (y_pred == 1) & (y_test == 0)\n",
    "FN = (y_pred == 0) & (y_test == 1)\n",
    "TN = (y_pred == 0) & (y_test == 0)\n",
    "\n",
    "total_utility = (\n",
    "    util_test['Utility_True_Positive'][TP].sum() +\n",
    "    util_test['Utility_False_Positive'][FP].sum() +\n",
    "    util_test['Utility_False_Negative'][FN].sum() +\n",
    "    util_test['Utility_True_Negative'][TN].sum()\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal Utility (financial impact) of model decisions: ${total_utility:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation of the Model Performance:**\n",
    "\n",
    "Your **classification report** shows the following metrics clearly:\n",
    "\n",
    "- **Precision (Positive class: \"1\" - Churn):** **0.57**  \n",
    "  Indicates that out of all customers predicted to churn, **57%** actually churned. The model is moderately good at correctly identifying actual churn cases.\n",
    "\n",
    "- **Recall (Positive class: \"1\" - Churn):** **0.55**  \n",
    "  Indicates that the model correctly detected **55%** of all customers who actually churned. There's room for improvement to better capture customers who might churn.\n",
    "\n",
    "- **F1-score (Positive class: \"1\" - Churn):** **0.56**  \n",
    "  Represents a balance between precision and recall. It's slightly above average, showing the model is fairly balanced but could benefit from further tuning.\n",
    "\n",
    "- **Accuracy:** **0.53** (53%)  \n",
    "  Slightly better than random (50%), suggesting that the model provides value but has considerable potential for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Insight:**\n",
    "The model shows basic effectiveness, correctly classifying slightly more than half of customers. While not yet highly accurate, the **real value** comes from the **utility-based financial evaluation**:  \n",
    "- Even moderate classification performance can yield significant business value if correctly prioritizing interventions based on Customer Lifetime Value (CLV).\n",
    "\n",
    "---\n",
    "\n",
    "## **Additional Test Examples (Predicting Churn for New Customer Profiles):**\n",
    "\n",
    "Here's Python code clearly demonstrating how to test new individual customer examples using your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Example 1: Likely to Churn\n",
      "Customer Example 2: Likely to Stay\n",
      "Customer Example 3: Likely to Churn\n"
     ]
    }
   ],
   "source": [
    "# Construct test examples using original categorical format\n",
    "test_examples_original = pd.DataFrame([\n",
    "    # Example 1: High CLV, Frequent buyer, positive engagement\n",
    "    {\n",
    "        'Gender': 'Male',\n",
    "        'Promotion_Response': 'Responded', # high engagement\n",
    "        'Email_Opt_In': True,\n",
    "        'Return_Ratio': 0.1,\n",
    "        'Purchase_Frequency': 0.8,\n",
    "        'CLV': 1500\n",
    "    },\n",
    "    # Example 2: Low CLV, high returns, poor engagement\n",
    "    {\n",
    "        'Gender': 'Female',\n",
    "        'Promotion_Response': 'Ignored', # low engagement\n",
    "        'Email_Opt_In': False,\n",
    "        'Return_Ratio': 0.7,\n",
    "        'Purchase_Frequency': 0.1,\n",
    "        'CLV': 200\n",
    "    },\n",
    "    # Example 3: Medium CLV, no engagement\n",
    "    {\n",
    "        'Gender': 'Other',\n",
    "        'Promotion_Response': 'Unsubscribed', \n",
    "        'Email_Opt_In': True,\n",
    "        'Return_Ratio': 0.3,\n",
    "        'Purchase_Frequency': 0.5,\n",
    "        'CLV': 700\n",
    "    }\n",
    "])\n",
    "\n",
    "# Encode categorical features to exactly match training data\n",
    "test_encoded = pd.get_dummies(test_examples_original, columns=['Gender', 'Promotion_Response', 'Email_Opt_In'])\n",
    "\n",
    "# Ensure all original features used during training are present, filling missing ones with zeros\n",
    "for col in X_train.columns:\n",
    "    if col not in test_encoded.columns:\n",
    "        test_encoded[col] = 0  # missing feature added with 0\n",
    "\n",
    "# Reorder columns to match exactly\n",
    "test_encoded = test_encoded[X_train.columns]\n",
    "\n",
    "# Now predict with the trained classifier\n",
    "example_predictions = clf.predict(test_encoded)\n",
    "\n",
    "# Display predictions clearly\n",
    "for i, pred in enumerate(example_predictions, 1):\n",
    "    result = 'Likely to Churn' if pred == 1 else 'Likely to Stay'\n",
    "    print(f\"Customer Example {i}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Next Steps (Recommendations for Improvement):**\n",
    "\n",
    "- **Tune model hyperparameters** (Decision Tree max_depth, min_samples_split, class weights).\n",
    "- Consider advanced methods (**Random Forest**, **Gradient Boosting**) for improved accuracy.\n",
    "- Incorporate additional features or refine existing ones.\n",
    "- Continuously refine **utility weights** based on actual campaign performance data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
