{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contextual Bandits vs. Full RL:**\n",
    "\n",
    "| Aspect                    | **Contextual Bandits**                           | **Full RL (e.g., Q-learning, DQN)**             |\n",
    "|---------------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| **Decision Focus**        | Single-step decisions (one interaction at a time) | Multi-step decisions (long-term sequences)      |\n",
    "| **State Transitions**     | No explicit state transitions                     | Explicit state transitions and environment states|\n",
    "| **Complexity**            | Simpler & quicker to train                        | More complex; takes longer to train             |\n",
    "| **Use-case suitability**  | Optimal when each decision is independent         | Optimal for long-term, sequential decisions     |\n",
    "\n",
    "---\n",
    "\n",
    "## **Is Contextual Bandit suitable for churn prediction explicitly?**\n",
    "\n",
    "In our scenario (deciding explicitly which customers to target with promotions):\n",
    "\n",
    "- Each decision (promotion or no promotion) is mostly independent.\n",
    "- You explicitly consider the customer's current state (context: churn probability, CLV, etc.).\n",
    "- You seek immediate reward optimization (send promotion → customer stays → immediate reward).\n",
    "\n",
    "---\n",
    "\n",
    "## **Our explicit Contextual Bandit setup (clearly stated):**\n",
    "\n",
    "- **Context (State)**: Customer features (CLV, purchase frequency, return ratio, etc.)\n",
    "- **Actions**: \n",
    "  - `0`: **No action**\n",
    "  - `1`: **Send promotion**\n",
    "- **Reward**:\n",
    "  - **Positive Reward** explicitly when customer remains active after intervention (economic gain = CLV − intervention cost).\n",
    "  - **Negative Reward** if intervention fails (loss = intervention cost).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicit Problem Restatement Clearly:**\n",
    "\n",
    "- **Context** (features of each customer):\n",
    "  - Churn Probability (from predictive model)\n",
    "  - Customer Lifetime Value (**CLV**)\n",
    "  - Promotion cost (fixed per customer)\n",
    "\n",
    "- **Actions**:\n",
    "  - `0`: **No action** (no cost, but potential churn)\n",
    "  - `1`: **Send promotion** (explicit intervention cost, but potential retention)\n",
    "\n",
    "- **Reward** explicitly defined as follows:\n",
    "  - If **customer retained** after promotion: reward = **CLV − promotion cost**\n",
    "  - If **customer churns** after promotion: reward = **− promotion cost**\n",
    "  - If **no action taken** and customer retained: reward = **CLV**\n",
    "  - If **no action taken** and customer churns: reward = **0**\n",
    "\n",
    "---\n",
    "\n",
    "## **Explicit Thompson Sampling Implementation (Contextual Bandit)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Initialization**: \n",
    "  - Explicitly initializes success/failure counts for each action (0 and 1).\n",
    "- **Action selection** (`select_action`):\n",
    "  - Explicitly samples from Beta distribution for each action.\n",
    "  - Explicitly chooses the action with the highest sampled probability (Thompson Sampling).\n",
    "- **Reward Update** (`update` method):\n",
    "  - Updates counts explicitly based on received reward (success if positive, failure if zero or negative).\n",
    "- **Simulation (`run_bandit` function)**:\n",
    "  - Iterates explicitly through each customer and selects actions.\n",
    "  - Explicitly calculates rewards based on the defined economics (CLV, churn probability, promotion cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ThompsonSamplingBandit:\n",
    "    def __init__(self):\n",
    "        # success and failure counts explicitly initialized\n",
    "        self.success_counts = np.ones(2)  # successes for each action\n",
    "        self.failure_counts = np.ones(2)  # failures for each action\n",
    "\n",
    "    def select_action(self):\n",
    "        # Explicitly select action using Thompson Sampling\n",
    "        sampled_probs = np.random.beta(self.success_counts, self.failure_counts)\n",
    "        return np.argmax(sampled_probs)\n",
    "\n",
    "    def update(self, action, reward, threshold=0):\n",
    "        # Explicitly update success/failure based on received reward\n",
    "        if reward > threshold:\n",
    "            self.success_counts[action] += 1\n",
    "        else:\n",
    "            self.failure_counts[action] += 1\n",
    "\n",
    "# Explicit function to run Thompson Sampling for churn intervention\n",
    "def run_bandit(data, cost_promotion):\n",
    "    bandit = ThompsonSamplingBandit()\n",
    "    history = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        churn_prob = row['Churn_Prob']\n",
    "        clv = row['CLV']\n",
    "\n",
    "        action = bandit.select_action()\n",
    "\n",
    "        # Explicitly simulate customer's response\n",
    "        customer_stays = np.random.rand() > churn_prob\n",
    "\n",
    "        if action == 1:  # Send promotion\n",
    "            reward = (clv - cost_promotion) if customer_stays else -cost_promotion\n",
    "        else:  # No action\n",
    "            reward = clv if customer_stays else 0\n",
    "\n",
    "        # Explicitly update bandit\n",
    "        bandit.update(action, reward)\n",
    "\n",
    "        history.append({\n",
    "            'Churn_Prob': churn_prob,\n",
    "            'CLV': clv,\n",
    "            'Action': 'Promotion' if action == 1 else 'No Action',\n",
    "            'Customer_Stays': customer_stays,\n",
    "            'Reward': reward\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Churn_Prob         CLV     Action  Customer_Stays      Reward\n",
      "0    0.694373  386.961240  Promotion            True  336.961240\n",
      "1    0.868660  100.981938  Promotion           False  -50.000000\n",
      "2    0.417101  164.802792  Promotion            True  114.802792\n",
      "3    0.829524  180.403407  Promotion           False  -50.000000\n",
      "4    0.466314  269.700620  Promotion            True  219.700620\n",
      "\n",
      "Summary explicitly stated:\n",
      "Action\n",
      "No Action    688.762266\n",
      "Promotion    867.086631\n",
      "Name: Reward, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('processed_customer_churn_data.csv')\n",
    "\n",
    " # For demonstration, assume churn probability is already predicted\n",
    " # Otherwise, use your trained model explicitly to add this column\n",
    "data['Churn_Prob'] = np.random.uniform(0.2, 0.9, size=len(data))\n",
    "\n",
    "cost_promotion = 50  # explicitly defined promotion cost\n",
    "\n",
    "results = run_bandit(data, cost_promotion)\n",
    "print(results.head())\n",
    "\n",
    "print(\"\\nSummary explicitly stated:\")\n",
    "print(results.groupby('Action')['Reward'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicit Explanation of the code clearly:**\n",
    "\n",
    "- **Initialization**: \n",
    "  - Explicitly initializes success/failure counts for each action (0 and 1).\n",
    "- **Action selection** (`select_action`):\n",
    "  - Explicitly samples from Beta distribution for each action.\n",
    "  - Explicitly chooses the action with the highest sampled probability (Thompson Sampling).\n",
    "- **Reward Update** (`update` method):\n",
    "  - Updates counts explicitly based on received reward (success if positive, failure if zero or negative).\n",
    "- **Simulation (`run_bandit` function)**:\n",
    "  - Iterates explicitly through each customer and selects actions.\n",
    "  - Explicitly calculates rewards based on the defined economics (CLV, churn probability, promotion cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicit Interpretation of Your Results:**\n",
    "\n",
    "### **First rows clearly explained:**\n",
    "\n",
    "- **Row 0:** Customer had **69% churn probability**. A promotion was sent, the customer stayed, and you got a positive reward (**CLV - promotion cost**).\n",
    "- **Row 1:** Customer had a **high churn probability (86%)**, received promotion, churned anyway, resulting in negative reward (**promotion cost lost**).\n",
    "- Similar logic explicitly applies to other rows.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (explicitly clear):**\n",
    "\n",
    "```text\n",
    "Action\n",
    "No Action    688.76 (average reward)\n",
    "Promotion    867.09 (average reward)\n",
    "```\n",
    "\n",
    "- **Promotion explicitly** provides significantly higher average economic rewards (**867.09**) compared to doing nothing (**688.76**).\n",
    "\n",
    "**Conclusion:**  \n",
    "The Contextual Bandit explicitly identified that actively intervening (promotions) is more economically beneficial on average for your specific customer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recommended Next Steps explicitly clear:**\n",
    "\n",
    "- **Refine the simulation explicitly** by adjusting:\n",
    "  - Promotion costs.\n",
    "  - Including real predicted churn probabilities explicitly from your Random Forest or SVM models (instead of randomly generated).\n",
    "  \n",
    "- **Run the bandit again explicitly** on refined data for robust insights.\n",
    "\n",
    "- **Implement exploration analysis explicitly** to visualize:\n",
    "  - How the bandit's strategy evolves explicitly over time.\n",
    "  - Economic performance explicitly over customer segments (high CLV vs. low CLV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
