{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integration of Economic Utility into Deep Reinforcement Learning Frameworks using Deep Contextual Bandits**\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology: Economic Optimization with Deep Contextual Bandits\n",
    "\n",
    "### 1. Overview of the Approach\n",
    "\n",
    "This research proposes extending reinforcement learning-based churn interventions by integrating **deep neural networks** into the contextual bandit framework, thus creating a **Deep Contextual Bandit (DCB)** model. The primary goal is to optimize promotional targeting by leveraging neural representations of customer contexts, combined explicitly with economic utility factors such as Customer Lifetime Value (CLV), churn probabilities, and promotional intervention costs. The DCB model addresses scalability, complexity of feature interactions, and dynamic customer behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Deep Contextual Bandit Formulation\n",
    "\n",
    "The contextual bandit problem involves selecting the optimal action $a_t$ at each decision step based on customer-specific context $x_t \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "- **Context:** A vector of customer features including behavioral (e.g., engagement score, return ratio), economic (CLV), and churn-risk indicators (predicted churn probability).\n",
    "- **Action Space:** $A = \\{0, 1\\}$, where 0 is \"No Action\" and 1 is \"Promotion\".\n",
    "- **Reward Function:** Defined economically, capturing CLV-adjusted utility and costs of interventions (similar to previous stages):\n",
    "  \n",
    "\n",
    "$$r_t =\n",
    "\\begin{cases}\n",
    "CLV_t - c, & \\text{if action = Promotion and customer stays} \\\\\n",
    "-c, & \\text{if action = Promotion and customer churns} \\\\\n",
    "CLV_t, & \\text{if action = No Action and customer stays} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "where $c$ denotes the fixed cost of sending a promotion.\n",
    "\n",
    "The agent's objective is maximizing cumulative expected reward, adjusting policy based on learned contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Neural Network Representation of Contexts\n",
    "\n",
    "Unlike classical contextual bandits, the **Deep Contextual Bandit** utilizes a neural network for context modeling:\n",
    "\n",
    "- **Embedding Layers:**  \n",
    "  Categorical variables (customer segments, promotional response categories) are embedded into dense vectors, capturing hidden relationships.\n",
    "  \n",
    "- **Hidden Layers:**  \n",
    "  Fully connected layers (multilayer perceptrons) with nonlinear activation functions (e.g., ReLU) model complex interactions among continuous and embedded features.\n",
    "\n",
    "- **Output Layer:**  \n",
    "  Produces predicted rewards (or expected utilities) for each action, guiding action selection.\n",
    "\n",
    "Formally, given a context $x_t$, the neural network outputs estimated action values $Q(x_t, a)$:\n",
    "\n",
    "$$Q(x_t, a) = \\text{NeuralNetwork}(x_t, a;\\theta)$$\n",
    "\n",
    "where $\\theta$ are neural network parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Deep Thompson Sampling or Neural UCB for Exploration\n",
    "\n",
    "To balance exploration-exploitation in the neural context, two methods are considered:\n",
    "\n",
    "- **Deep Thompson Sampling:**  \n",
    "  Maintains Bayesian uncertainty estimates on network parameters $\\theta$. At each step, parameters are sampled from posterior distributions, and actions chosen according to sampled $Q$-values:\n",
    "\n",
    "  $$a_t = \\arg\\max_{a} Q(x_t, a|\\theta_t^{sampled})$$\n",
    "  \n",
    "- **Neural Upper Confidence Bound (Neural UCB):**  \n",
    "  Computes an exploration bonus from neural uncertainty, choosing actions as follows:\n",
    "\n",
    "   $$a_t = \\arg\\max_{a} \\left[Q(x_t, a|\\theta_t) + \\alpha \\cdot U(x_t, a)\\right]$$\n",
    "  \n",
    "  where $U(x_t, a)$ represents the uncertainty estimate, and $\\alpha$ controls exploration strength.\n",
    "\n",
    "Both approaches inherently promote exploration of potentially profitable actions while converging towards optimal strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training and Optimization Procedure\n",
    "\n",
    "The training process follows a structured sequence:\n",
    "\n",
    "- **Data Generation:**  \n",
    "  Actions are executed, customer responses observed (simulated or from historical data), and rewards calculated.\n",
    "\n",
    "- **Neural Network Update:**  \n",
    "  Using collected data, the neural network parameters $\\theta$ are optimized via gradient-based methods (e.g., Adam optimizer) to minimize loss between predicted and observed rewards:\n",
    "\n",
    "  $$L(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}(r_i - Q(x_i, a_i|\\theta))^2$$\n",
    "  \n",
    "- **Bayesian or Uncertainty Updates:**  \n",
    "  For Deep Thompson Sampling, update posterior parameter distributions. For Neural UCB, update uncertainty estimates using Bayesian linear regression approximations or variational inference.\n",
    "\n",
    "This cycle continues iteratively, refining the policy adaptively.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Evaluation and Deployment Strategy\n",
    "\n",
    "- **Offline Evaluation:**  \n",
    "  Conducted via historical dataset and economic metrics (profit, incremental retention utility), including standard reinforcement learning metrics (cumulative reward, regret reduction).\n",
    "\n",
    "- **Segment-Level Performance:**  \n",
    "  Assessed by customer groups (low/medium/high CLV and churn risk) to confirm strategic targeting and economic alignment.\n",
    "\n",
    "- **Online Deployment Preparation:**  \n",
    "  Prepares the model for real-time decision-making integration into CRM pipelines, allowing live updates and continuous learning from new customer interactions.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Practical and Business Value\n",
    "\n",
    "The DCB framework explicitly aligns technical optimization with business strategy by:\n",
    "\n",
    "- Enhancing economic return through precise and adaptive promotion targeting.\n",
    "- Reducing promotional waste and improving budget efficiency.\n",
    "- Enabling personalized, scalable retention strategies dynamically tuned to real-time customer behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Methodological Contributions\n",
    "\n",
    "- Integrated deep neural modeling of complex customer contexts into the contextual bandit framework.\n",
    "- Employed Deep Thompson Sampling or Neural UCB for adaptive decision-making with economic utility objectives.\n",
    "- Proposed a practical training and evaluation procedure ensuring alignment with business outcomes and economic rationality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network Representation of Customer Contexts (Implementation Example)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.5858\n",
      "Epoch 2, Loss: 2.8188\n",
      "Epoch 3, Loss: 2.4008\n",
      "Epoch 4, Loss: 2.2690\n",
      "Epoch 5, Loss: 2.2195\n",
      "Epoch 6, Loss: 2.1993\n",
      "Epoch 7, Loss: 2.1765\n",
      "Epoch 8, Loss: 2.1741\n",
      "Epoch 9, Loss: 2.1586\n",
      "Epoch 10, Loss: 2.1496\n",
      "Epoch 11, Loss: 2.1358\n",
      "Epoch 12, Loss: 2.1299\n",
      "Epoch 13, Loss: 2.1218\n",
      "Epoch 14, Loss: 2.1282\n",
      "Epoch 15, Loss: 2.1177\n",
      "Epoch 16, Loss: 2.1225\n",
      "Epoch 17, Loss: 2.1179\n",
      "Epoch 18, Loss: 2.1043\n",
      "Epoch 19, Loss: 2.1146\n",
      "Epoch 20, Loss: 2.1122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return_Ratio</th>\n",
       "      <th>Purchase_Frequency</th>\n",
       "      <th>Engagement_Score</th>\n",
       "      <th>CLV</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Promotion_Response_Responded</th>\n",
       "      <th>Promotion_Response_Unsubscribed</th>\n",
       "      <th>Email_Opt_In_Score</th>\n",
       "      <th>Q_NoAction</th>\n",
       "      <th>Q_Promotion</th>\n",
       "      <th>Selected_Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.396829</td>\n",
       "      <td>-0.113941</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.222855</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.465509</td>\n",
       "      <td>0.507648</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127745</td>\n",
       "      <td>-0.199646</td>\n",
       "      <td>0.114272</td>\n",
       "      <td>-0.291221</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.503785</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109187</td>\n",
       "      <td>-0.197047</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.179608</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.524349</td>\n",
       "      <td>0.529532</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134001</td>\n",
       "      <td>-0.199136</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.238356</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.488522</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.255446</td>\n",
       "      <td>0.092976</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>0.119375</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.457727</td>\n",
       "      <td>0.443353</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Return_Ratio  Purchase_Frequency  Engagement_Score       CLV  Gender_Male  \\\n",
       "0     -0.396829           -0.113941         -0.618242 -0.222855    -0.708168   \n",
       "1     -0.127745           -0.199646          0.114272 -0.291221    -0.708168   \n",
       "2     -0.109187           -0.197047         -0.618242 -0.179608     1.412095   \n",
       "3      0.134001           -0.199136         -0.618242 -0.238356    -0.708168   \n",
       "4     -0.255446            0.092976         -0.618242  0.119375     1.412095   \n",
       "\n",
       "   Promotion_Response_Responded  Promotion_Response_Unsubscribed  \\\n",
       "0                     -0.714545                         1.330445   \n",
       "1                     -0.714545                        -0.751628   \n",
       "2                     -0.714545                        -0.751628   \n",
       "3                     -0.714545                        -0.751628   \n",
       "4                     -0.714545                         1.330445   \n",
       "\n",
       "   Email_Opt_In_Score  Q_NoAction  Q_Promotion Selected_Action  \n",
       "0            0.943588    0.465509     0.507648       Promotion  \n",
       "1            0.943588    0.503785     0.491071       No Action  \n",
       "2           -1.059784    0.524349     0.529532       Promotion  \n",
       "3           -1.059784    0.488522     0.513325       Promotion  \n",
       "4            0.943588    0.457727     0.443353       No Action  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load and prepare the data\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"processed_customer_churn_data.csv\")\n",
    "\n",
    "features = [\n",
    "    'Return_Ratio', 'Purchase_Frequency', 'Engagement_Score', 'CLV',\n",
    "    'Gender_Male', 'Promotion_Response_Responded',\n",
    "    'Promotion_Response_Unsubscribed', 'Email_Opt_In_Score'\n",
    "]\n",
    "target = 'Target_Churn'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, _, _ = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Dummy Q-values as placeholder\n",
    "y_dummy = np.random.rand(X_train.shape[0], 2)\n",
    "y_train_tensor = torch.tensor(y_dummy, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define PyTorch MLP model\n",
    "# -------------------------------\n",
    "class ChurnMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ChurnMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)  # Two Q-values: for No Action and Promotion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = ChurnMLP(input_dim=X_train.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Train the model\n",
    "# -------------------------------\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(X_train_tensor.size()[0])\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, X_train_tensor.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Predict Q-values for test set\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    q_values_test = model(X_test_tensor)\n",
    "    actions = torch.argmax(q_values_test, dim=1).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return_Ratio</th>\n",
       "      <th>Purchase_Frequency</th>\n",
       "      <th>Engagement_Score</th>\n",
       "      <th>CLV</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Promotion_Response_Responded</th>\n",
       "      <th>Promotion_Response_Unsubscribed</th>\n",
       "      <th>Email_Opt_In_Score</th>\n",
       "      <th>Q_NoAction</th>\n",
       "      <th>Q_Promotion</th>\n",
       "      <th>Selected_Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.396829</td>\n",
       "      <td>-0.113941</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.222855</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.465509</td>\n",
       "      <td>0.507648</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127745</td>\n",
       "      <td>-0.199646</td>\n",
       "      <td>0.114272</td>\n",
       "      <td>-0.291221</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.503785</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109187</td>\n",
       "      <td>-0.197047</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.179608</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.524349</td>\n",
       "      <td>0.529532</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134001</td>\n",
       "      <td>-0.199136</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.238356</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.488522</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.255446</td>\n",
       "      <td>0.092976</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>0.119375</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.457727</td>\n",
       "      <td>0.443353</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.939821</td>\n",
       "      <td>-0.234457</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.288333</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.456523</td>\n",
       "      <td>0.383056</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.396829</td>\n",
       "      <td>1.327229</td>\n",
       "      <td>-1.350757</td>\n",
       "      <td>0.494731</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.539814</td>\n",
       "      <td>0.551357</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.268497</td>\n",
       "      <td>-0.209579</td>\n",
       "      <td>0.846787</td>\n",
       "      <td>-0.260639</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>1.399493</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.565612</td>\n",
       "      <td>0.445365</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.396829</td>\n",
       "      <td>0.071957</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.276992</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.494927</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.118775</td>\n",
       "      <td>-0.218553</td>\n",
       "      <td>0.846787</td>\n",
       "      <td>-0.296703</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>1.399493</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.559254</td>\n",
       "      <td>0.553237</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.240424</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.128709</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.509163</td>\n",
       "      <td>0.530851</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.207247</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-1.350757</td>\n",
       "      <td>-0.147797</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.467824</td>\n",
       "      <td>0.547730</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.944796</td>\n",
       "      <td>-0.220433</td>\n",
       "      <td>0.846787</td>\n",
       "      <td>-0.256213</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>1.399493</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.323158</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.118775</td>\n",
       "      <td>-0.195821</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>-0.292839</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.475620</td>\n",
       "      <td>0.512749</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.288026</td>\n",
       "      <td>1.337003</td>\n",
       "      <td>-0.618242</td>\n",
       "      <td>5.018949</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.271403</td>\n",
       "      <td>0.212562</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.270501</td>\n",
       "      <td>-0.226543</td>\n",
       "      <td>0.114272</td>\n",
       "      <td>-0.278429</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.485219</td>\n",
       "      <td>0.482556</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.184497</td>\n",
       "      <td>-0.188597</td>\n",
       "      <td>0.846787</td>\n",
       "      <td>-0.196905</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>1.399493</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.442736</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.199265</td>\n",
       "      <td>-0.145709</td>\n",
       "      <td>0.114272</td>\n",
       "      <td>-0.289915</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.476162</td>\n",
       "      <td>0.487573</td>\n",
       "      <td>Promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.111993</td>\n",
       "      <td>-0.184101</td>\n",
       "      <td>1.579301</td>\n",
       "      <td>-0.245016</td>\n",
       "      <td>-0.708168</td>\n",
       "      <td>1.399493</td>\n",
       "      <td>-0.751628</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>0.468937</td>\n",
       "      <td>0.444768</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.118775</td>\n",
       "      <td>-0.179253</td>\n",
       "      <td>-1.350757</td>\n",
       "      <td>-0.274315</td>\n",
       "      <td>1.412095</td>\n",
       "      <td>-0.714545</td>\n",
       "      <td>1.330445</td>\n",
       "      <td>-1.059784</td>\n",
       "      <td>0.558813</td>\n",
       "      <td>0.479509</td>\n",
       "      <td>No Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Return_Ratio  Purchase_Frequency  Engagement_Score       CLV  Gender_Male  \\\n",
       "0      -0.396829           -0.113941         -0.618242 -0.222855    -0.708168   \n",
       "1      -0.127745           -0.199646          0.114272 -0.291221    -0.708168   \n",
       "2      -0.109187           -0.197047         -0.618242 -0.179608     1.412095   \n",
       "3       0.134001           -0.199136         -0.618242 -0.238356    -0.708168   \n",
       "4      -0.255446            0.092976         -0.618242  0.119375     1.412095   \n",
       "5       2.939821           -0.234457         -0.618242 -0.288333    -0.708168   \n",
       "6      -0.396829            1.327229         -1.350757  0.494731    -0.708168   \n",
       "7      -0.268497           -0.209579          0.846787 -0.260639    -0.708168   \n",
       "8      -0.396829            0.071957         -0.618242 -0.276992    -0.708168   \n",
       "9      -0.118775           -0.218553          0.846787 -0.296703     1.412095   \n",
       "10     -0.240424            0.021030         -0.618242 -0.128709    -0.708168   \n",
       "11     -0.207247           -0.194133         -1.350757 -0.147797    -0.708168   \n",
       "12      7.944796           -0.220433          0.846787 -0.256213     1.412095   \n",
       "13     -0.118775           -0.195821         -0.618242 -0.292839    -0.708168   \n",
       "14     -0.288026            1.337003         -0.618242  5.018949     1.412095   \n",
       "15      0.270501           -0.226543          0.114272 -0.278429    -0.708168   \n",
       "16     -0.184497           -0.188597          0.846787 -0.196905    -0.708168   \n",
       "17     -0.199265           -0.145709          0.114272 -0.289915     1.412095   \n",
       "18     -0.111993           -0.184101          1.579301 -0.245016    -0.708168   \n",
       "19     -0.118775           -0.179253         -1.350757 -0.274315     1.412095   \n",
       "\n",
       "    Promotion_Response_Responded  Promotion_Response_Unsubscribed  \\\n",
       "0                      -0.714545                         1.330445   \n",
       "1                      -0.714545                        -0.751628   \n",
       "2                      -0.714545                        -0.751628   \n",
       "3                      -0.714545                        -0.751628   \n",
       "4                      -0.714545                         1.330445   \n",
       "5                      -0.714545                         1.330445   \n",
       "6                      -0.714545                         1.330445   \n",
       "7                       1.399493                        -0.751628   \n",
       "8                      -0.714545                         1.330445   \n",
       "9                       1.399493                        -0.751628   \n",
       "10                     -0.714545                        -0.751628   \n",
       "11                     -0.714545                         1.330445   \n",
       "12                      1.399493                        -0.751628   \n",
       "13                     -0.714545                         1.330445   \n",
       "14                     -0.714545                         1.330445   \n",
       "15                     -0.714545                        -0.751628   \n",
       "16                      1.399493                        -0.751628   \n",
       "17                     -0.714545                        -0.751628   \n",
       "18                      1.399493                        -0.751628   \n",
       "19                     -0.714545                         1.330445   \n",
       "\n",
       "    Email_Opt_In_Score  Q_NoAction  Q_Promotion Selected_Action  \n",
       "0             0.943588    0.465509     0.507648       Promotion  \n",
       "1             0.943588    0.503785     0.491071       No Action  \n",
       "2            -1.059784    0.524349     0.529532       Promotion  \n",
       "3            -1.059784    0.488522     0.513325       Promotion  \n",
       "4             0.943588    0.457727     0.443353       No Action  \n",
       "5             0.943588    0.456523     0.383056       No Action  \n",
       "6            -1.059784    0.539814     0.551357       Promotion  \n",
       "7            -1.059784    0.565612     0.445365       No Action  \n",
       "8             0.943588    0.459022     0.494927       Promotion  \n",
       "9            -1.059784    0.559254     0.553237       No Action  \n",
       "10           -1.059784    0.509163     0.530851       Promotion  \n",
       "11           -1.059784    0.467824     0.547730       Promotion  \n",
       "12           -1.059784    0.323158     0.618645       Promotion  \n",
       "13            0.943588    0.475620     0.512749       Promotion  \n",
       "14            0.943588    0.271403     0.212562       No Action  \n",
       "15            0.943588    0.485219     0.482556       No Action  \n",
       "16           -1.059784    0.561400     0.442736       No Action  \n",
       "17            0.943588    0.476162     0.487573       Promotion  \n",
       "18            0.943588    0.468937     0.444768       No Action  \n",
       "19           -1.059784    0.558813     0.479509       No Action  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5. Preview decision output\n",
    "# -------------------------------\n",
    "results = pd.DataFrame(X_test[:20], columns=features)\n",
    "results['Q_NoAction'] = q_values_test[:20, 0].numpy()\n",
    "results['Q_Promotion'] = q_values_test[:20, 1].numpy()\n",
    "results['Selected_Action'] = np.where(actions[:20] == 1, 'Promotion', 'No Action')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table displays the **output of a PyTorch-based neural network** (multi-layer perceptron) used to estimate **Q-values** for two possible actions: `\"NoAction\"` and `\"Promotion\"` in a **contextual bandit framework** for churn-sensitive marketing.\n",
    "\n",
    "### Context Overview:\n",
    "Each row corresponds to an individual customer and includes:\n",
    "- Normalized customer **features** (e.g., `Return_Ratio`, `CLV`, `Email_Opt_In_Score`)\n",
    "- Model-predicted **Q-values** for each action (`Q_NoAction` and `Q_Promotion`)\n",
    "\n",
    "These Q-values represent the **expected utility or reward** the agent anticipates from choosing each respective action in the given context.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of Results:\n",
    "\n",
    "| Feature Type        | Insight |\n",
    "|---------------------|---------|\n",
    "| **Features**        | Customer behavior and demographic inputs have been **standardized** to zero mean and unit variance for stable model learning. |\n",
    "| **Q_NoAction / Q_Promotion** | These are the output logits from the neural network that estimate the **value of taking each action**. The agent chooses the action with the higher Q-value. |\n",
    "| **Example (Row 0)** | The Q-values are `0.4655` for NoAction and `0.5076` for Promotion — the model would select **Promotion** for this customer. |\n",
    "| **Example (Row 5)** | Q-values are `0.4565` (NoAction) and `0.3830` (Promotion) — **NoAction** is the optimal decision here. |\n",
    "\n",
    "---\n",
    "\n",
    "### What This Means:\n",
    "- The neural model **learned patterns** in the feature space to distinguish which customers are more likely to benefit (in economic terms) from promotional interventions.\n",
    "- The **variation in Q-values** shows personalized decision-making instead of static rules — a key strength of contextual bandit methods.\n",
    "- This Q-estimation is foundational for a **deep contextual bandit agent**, which balances **exploration vs. exploitation** during retention campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Interpretation & Next Steps:**\n",
    "\n",
    "- This neural network approach captures complex interactions between customer features, predicting Q-values that represent the expected economic utility of each action.\n",
    "- Integrate this neural network within your Thompson Sampling or Neural UCB algorithms for fully adaptive action selection and continuous learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Future Improvements:**\n",
    "\n",
    "- Replace dummy targets with actual reward signals from the bandit's interactions.\n",
    "- Incorporate embedding layers for categorical features (e.g., gender, email responsiveness).\n",
    "- Implement Bayesian or dropout-based uncertainty estimation to enable Deep Thompson Sampling or Neural UCB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
